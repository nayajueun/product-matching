{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import util, losses\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_file_dir = './data/computers_train/computers_medium_large.json.gz'\n",
    "ft_valid_file_dir = './data/computers_valid/computers_valid_medium.csv'\n",
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ./data/computers_train/computers_medium_large.json.gz does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nayaj\\Desktop\\git-project\\product-matching\\main2 copy.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nayaj/Desktop/git-project/product-matching/main2%20copy.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# finetuning bert model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nayaj/Desktop/git-project/product-matching/main2%20copy.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m large_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(fine_tuning_file_dir, compression\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgzip\u001b[39;49m\u001b[39m'\u001b[39;49m, lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nayaj/Desktop/git-project/product-matching/main2%20copy.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m large_df \u001b[39m=\u001b[39m large_df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mid_left\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mid_right\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcluster_id_left\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcluster_id_right\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39midentifiers_left\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39midentifiers_right\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nayaj/Desktop/git-project/product-matching/main2%20copy.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m valid_ids \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(ft_valid_file_dir)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\json\\_json.py:733\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    731\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[0;32m    734\u001b[0m     path_or_buf,\n\u001b[0;32m    735\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[0;32m    736\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[0;32m    737\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    738\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[0;32m    739\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[0;32m    740\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[0;32m    741\u001b[0m     numpy\u001b[39m=\u001b[39;49mnumpy,\n\u001b[0;32m    742\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[0;32m    743\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[0;32m    744\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    745\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[0;32m    746\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    747\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    748\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m    749\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    750\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[0;32m    751\u001b[0m )\n\u001b[0;32m    753\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[0;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\json\\_json.py:818\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlines:\n\u001b[0;32m    816\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnrows can only be passed if lines=True\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 818\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data_from_filepath(filepath_or_buffer)\n\u001b[0;32m    819\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\json\\_json.py:874\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    866\u001b[0m     filepath_or_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[0;32m    867\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    868\u001b[0m     \u001b[39misinstance\u001b[39m(filepath_or_buffer, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    869\u001b[0m     \u001b[39mand\u001b[39;00m filepath_or_buffer\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[0;32m    873\u001b[0m ):\n\u001b[1;32m--> 874\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfilepath_or_buffer\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m filepath_or_buffer\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File ./data/computers_train/computers_medium_large.json.gz does not exist"
     ]
    }
   ],
   "source": [
    "# finetuning bert model\n",
    "large_df = pd.read_json(fine_tuning_file_dir, compression='gzip', lines=True)\n",
    "large_df = large_df.drop(columns=['id_left', 'id_right', 'cluster_id_left', 'cluster_id_right', 'identifiers_left', 'identifiers_right'])\n",
    "valid_ids = pd.read_csv(ft_valid_file_dir)\n",
    "ft_train_df = large_df.loc[~large_df['pair_id'].isin(valid_ids['pair_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_similarity(model, s_pair):\n",
    "    if s_pair[0] == None or s_pair[1] == None:\n",
    "        return None \n",
    "\n",
    "    if s_pair[0] == s_pair[1]:\n",
    "        return 1.0\n",
    "    \n",
    "    embds = model.encode(s_pair)\n",
    "    sim_score = util.cos_sim(embds[0], embds[1]).item()\n",
    "    return sim_score\n",
    "\n",
    "def get_similarity_keyval(dic1, dic2):\n",
    "    key_set = set()\n",
    "    tot_sim = 0\n",
    "    for key, value1 in zip(dic1.keys(), dic1.values()):\n",
    "        if key in dic2:\n",
    "            value2 = dic2[key]\n",
    "            if value2 == value1:\n",
    "                tot_sim += 1\n",
    "\n",
    "        key_set.add(key)\n",
    "\n",
    "    for key2 in dic2:\n",
    "        key_set.add(key2)\n",
    "\n",
    "    return tot_sim / len(key_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json('./data/computers_train/computers_train_medium.json.gz', compression='gzip', lines=True)\n",
    "data_df = data_df.drop(columns=['id_left', 'id_right', 'cluster_id_left', 'cluster_id_right', 'identifiers_left', 'identifiers_right'])\n",
    "\n",
    "valid_ids = pd.read_csv('./data/computers_valid/computers_valid_medium.csv')\n",
    "\n",
    "train_df = data_df.loc[~data_df['pair_id'].isin(valid_ids['pair_id'])]\n",
    "valid_df = data_df.loc[data_df['pair_id'].isin(valid_ids['pair_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pair_no = 0\n",
    "idx_to_pair_no = {}\n",
    "description_list= []\n",
    "for left, right in zip(train_df['description_left'], train_df['description_right']):\n",
    "    if left != None and right != None:\n",
    "        pair_no += 1\n",
    "        idx_to_pair_no[idx] = pair_no\n",
    "        description_list.append(left)\n",
    "        description_list.append(right)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8290"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20874070e-01,  1.69541880e-01,  1.97632685e-01, ...,\n",
       "         3.76053620e-03, -1.01542033e-01,  9.13804471e-02],\n",
       "       [ 1.13327131e-01,  1.18805088e-01,  1.91182092e-01, ...,\n",
       "        -2.79869908e-03, -1.02162220e-01,  5.41441888e-02],\n",
       "       [-9.08283368e-02,  3.79040428e-02,  1.22852005e-01, ...,\n",
       "         8.88176113e-02,  3.68592963e-02,  2.26478856e-02],\n",
       "       ...,\n",
       "       [-6.76703528e-02,  2.83360690e-01, -9.54265371e-02, ...,\n",
       "         8.91093984e-02, -2.48319328e-01,  2.23582402e-01],\n",
       "       [ 6.41614869e-02,  1.75788209e-01, -2.01825052e-01, ...,\n",
       "        -6.47189663e-05, -2.96221256e-01,  2.78991967e-01],\n",
       "       [-6.76703528e-02,  2.83360690e-01, -9.54265371e-02, ...,\n",
       "         8.91093984e-02, -2.48319328e-01,  2.23582402e-01]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model_eng.encode(description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_similarity_vectorizer(value_left, value_right, vectorizer):\n",
    "    if value_left == None or value_right == None:\n",
    "        return None\n",
    "\n",
    "    if value_left == value_right:\n",
    "        return 1.0\n",
    "    tf_idf_left = vectorizer.transform([value_left])\n",
    "    tf_idf_right = vectorizer.transform([value_right])\n",
    "    sim = cosine_similarity(tf_idf_left, tf_idf_right)[0][0]\n",
    "    return sim\n",
    "\n",
    "\n",
    "def compute_features(df):\n",
    "    # TODO: title, brand and keyValue using Tfidf / encode all sentences at once to sbert\n",
    "    num_df = pd.DataFrame()\n",
    "    text_df = pd.concat([df['title_left'], df['title_right']])\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer = vectorizer.fit(text_df.values)\n",
    "    num_df['title_sim'] = df.apply(\n",
    "        lambda x: get_similarity_vectorizer(x['title_left'], x['title_right'], vectorizer), axis=1)\n",
    "    print(\"title done\")\n",
    "\n",
    "    text_df = pd.concat([df['description_left'], df['description_right']])\n",
    "    text_df = text_df.dropna(how=\"any\", axis=0)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer = vectorizer.fit(text_df.values)\n",
    "    num_df['description_sim'] = df.apply(\n",
    "        lambda x: get_similarity_vectorizer(x['description_left'], x['description_right'], vectorizer), axis=1)\n",
    "    print(\"descrition done\")\n",
    "\n",
    "    text_df = pd.concat([df['brand_left'], df['brand_right']])\n",
    "    text_df = text_df.dropna(how=\"any\", axis=0)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer = vectorizer.fit(text_df.values)\n",
    "    num_df['brand_sim'] = df.apply(lambda x: get_similarity_vectorizer(x['brand_left'], x['brand_right'], vectorizer), axis=1)\n",
    "\n",
    "    num_df['label'] = df['label']\n",
    "    return num_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title done\n",
      "descrition done\n"
     ]
    }
   ],
   "source": [
    "num_train_df = compute_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title done\n",
      "descrition done\n"
     ]
    }
   ],
   "source": [
    "num_valid_df = compute_features(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_sim</th>\n",
       "      <th>description_sim</th>\n",
       "      <th>brand_sim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.574667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.586431</td>\n",
       "      <td>0.039588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.641244</td>\n",
       "      <td>0.140730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.662503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.212899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8089</th>\n",
       "      <td>0.144609</td>\n",
       "      <td>0.109454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8091</th>\n",
       "      <td>0.295581</td>\n",
       "      <td>0.361471</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8092</th>\n",
       "      <td>0.343734</td>\n",
       "      <td>0.207120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8093</th>\n",
       "      <td>0.304371</td>\n",
       "      <td>0.189632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6475 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      title_sim  description_sim  brand_sim  label\n",
       "3      0.574667              NaN        NaN      1\n",
       "4      0.586431         0.039588        1.0      1\n",
       "5      0.641244         0.140730        NaN      1\n",
       "6      0.662503              NaN        NaN      1\n",
       "7      0.212899              NaN        NaN      1\n",
       "...         ...              ...        ...    ...\n",
       "8089   0.144609         0.109454        0.0      0\n",
       "8090   0.000000         0.000000        0.0      0\n",
       "8091   0.295581         0.361471        1.0      0\n",
       "8092   0.343734         0.207120        1.0      0\n",
       "8093   0.304371         0.189632        1.0      0\n",
       "\n",
       "[6475 rows x 4 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "#isolate the x and y variables\n",
    "y_train = np.array(num_train_df['label'])\n",
    "X_train = num_train_df.drop(columns=[\"label\"])\n",
    "    \n",
    "y_test = np.array(num_valid_df['label'])\n",
    "X_test = num_valid_df.drop(columns=[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tYes-auc:0.87963\n",
      "[50]\tYes-auc:0.91091\n",
      "[100]\tYes-auc:0.91172\n",
      "[150]\tYes-auc:0.90917\n",
      "[199]\tYes-auc:0.90705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      1267\n",
      "           1       0.77      0.67      0.72       352\n",
      "\n",
      "    accuracy                           0.88      1619\n",
      "   macro avg       0.84      0.81      0.82      1619\n",
      "weighted avg       0.88      0.88      0.88      1619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create xgboost matrices\n",
    "Train = xgb.DMatrix(X_train, label = y_train)\n",
    "Test = xgb.DMatrix(X_test, label = y_test)\n",
    "\n",
    "#set the parameters\n",
    "parameters = {'learning_rate': 0.3,\n",
    "               'max_depth': 2,\n",
    "               'colsample_bytree': 1,\n",
    "               'subsample': 1,\n",
    "               'min_child_weight': 1,\n",
    "               'gamma': 0, \n",
    "               'random_state': 1500,\n",
    "               'eval_metric': \"auc\",\n",
    "               'objective': \"binary:logistic\"}\n",
    "\n",
    "\n",
    "model = xgb.train(params = parameters,\n",
    "                dtrain = Train,\n",
    "                num_boost_round = 200,\n",
    "                evals = [(Test, \"Yes\")],\n",
    "                verbose_eval = 50)\n",
    "\n",
    "\n",
    "#PRedictions\n",
    "pred = model.predict(Test)\n",
    "pred = np.where(pred > 0.5, 1, 0)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, pred)\n",
    "report = classification_report(y_test, pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
